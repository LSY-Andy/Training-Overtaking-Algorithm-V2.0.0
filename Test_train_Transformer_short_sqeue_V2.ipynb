{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 03:45:31.123394: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.9.0\n",
      "INFO:tensorflow:Using MirroredStrategy with devices ('/job:localhost/replica:0/task:0/device:GPU:0', '/job:localhost/replica:0/task:0/device:GPU:1')\n",
      "Number of devices: 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 03:45:32.608466: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 AVX512F AVX512_VNNI AVX512_BF16 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-03-30 03:45:33.570588: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 0\n",
      "2024-03-30 03:45:33.570728: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 22302 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:03:00.0, compute capability: 8.6\n",
      "2024-03-30 03:45:33.571471: I tensorflow/core/common_runtime/gpu/gpu_process_state.cc:222] Using CUDA malloc Async allocator for GPU: 1\n",
      "2024-03-30 03:45:33.571557: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1532] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 22302 MB memory:  -> device: 1, name: NVIDIA GeForce RTX 3090, pci bus id: 0000:84:00.0, compute capability: 8.6\n"
     ]
    }
   ],
   "source": [
    "import tensorflow\n",
    "print(tensorflow.__version__)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.utils import Sequence\n",
    "\n",
    "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
    "    tf.config.experimental.set_memory_growth(gpu, True)\n",
    "\n",
    "\n",
    "# from tensorflow.python.keras.models import Model, load_model\n",
    "# from tensorflow.python.keras.layers import Dense, Input, Masking, Embedding, concatenate\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.layers import Input, GRU, Dense, TimeDistributed, Embedding,Masking, concatenate\n",
    "\n",
    "import os\n",
    "import re\n",
    "import gc\n",
    "\n",
    "import os\n",
    "os.environ['TF_ENABLE_GPU_GARBAGE_COLLECTION'] = 'true'\n",
    "os.environ['TF_GPU_ALLOCATOR'] = 'cuda_malloc_async'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'  # 0 = all messages are logged (default behavior)\n",
    "                                          # 1 = INFO messages are not printed\n",
    "                                          # 2 = INFO and WARNING messages are not printed\n",
    "                                          # 3 = INFO, WARNING, and ERROR messages are not printed\n",
    "            \n",
    "# 创建一个 MirroredStrategy.\n",
    "strategy = tf.distribute.MirroredStrategy()\n",
    "\n",
    "print('Number of devices: {}'.format(strategy.num_replicas_in_sync))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X_data是模型输入；Y_data是模型标签\n",
    "\n",
    "# class MemoryDataGenerator(Sequence):\n",
    "#     def __init__(self, X_data, Y_data, batch_size):\n",
    "#         self.X_data = X_data  # 模型的第一个输入\n",
    "#         self.Y_data = Y_data  # 模型的第二个输入，同时也是模型的输出标签\n",
    "#         self.batch_size = batch_size\n",
    "    \n",
    "#     def __len__(self):\n",
    "#         return np.ceil(len(self.X_data) / self.batch_size).astype(int)\n",
    "    \n",
    "#     def __getitem__(self, index):\n",
    "#         batch_start = index * self.batch_size\n",
    "#         batch_end = (index + 1) * self.batch_size\n",
    "        \n",
    "#         batch_X = self.X_data[batch_start:batch_end]\n",
    "#         batch_Y = self.Y_data[batch_start:batch_end]\n",
    "        \n",
    "#         # 注意，这里返回两个输入和一个输出（输出即第二个输入）\n",
    "#         return [np.array(batch_X), np.array(batch_Y)], np.array(batch_Y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【1083版本】\n",
    "# X_data是模型输入；Y_data是模型标签\n",
    "class MemoryDataGenerator(Sequence):\n",
    "    def __init__(self, X_data, Y_data, batch_size):\n",
    "        self.X_data = X_data\n",
    "        self.Y_data = Y_data\n",
    "        self.batch_size = batch_size\n",
    "    \n",
    "    def __len__(self):\n",
    "        # 计算在给定数据集大小下的批次数量\n",
    "        return int(np.ceil(len(self.X_data) / self.batch_size))\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        # 生成批次的数据索引\n",
    "        batch_start = index * self.batch_size\n",
    "        batch_end = (index + 1) * self.batch_size\n",
    "        \n",
    "        # 根据索引获取当前批次的数据\n",
    "        batch_X = self.X_data[batch_start:batch_end]\n",
    "        batch_Y = self.Y_data[batch_start:batch_end]\n",
    "        \n",
    "        # 对当前批次的数据进行填充\n",
    "        # 现在感觉并不需要\n",
    "        # batch_X_padded = tensorflow.keras.preprocessing.sequence.pad_sequences(batch_X, padding=\"post\", value=special_value, dtype='float32')\n",
    "        \n",
    "        # batch_Y_padded = tensorflow.keras.preprocessing.sequence.pad_sequences(batch_Y, padding=\"post\", value=special_value, dtype='float32')\n",
    "        \n",
    "        return np.array(batch_X), np.array(batch_Y)\n",
    "        # 目的: 确保数据以适合模型处理的格式（如float32）被送入训练过程。\n",
    "        # 操作: 将填充后的数据转换为NumPy数组，并确保数据类型为float32，这是大多数深度学习框架进行计算的首选格式。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_data(folder):\n",
    "\n",
    "    # use regular expression to filter files\n",
    "    # file's name start with 'ML'\n",
    "    check_name = re.compile('^ML')\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    # due to different operating system has different way in keeping files, here I would like to read files in sorted by name order and say it explicitly\n",
    "    # the reason why must sorted by name will be explained later\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "\n",
    "        files = os.path.join(folder, filename)\n",
    "\n",
    "        # check whether the name match the regular expression and actually is a file\n",
    "        # 对文件名进行顺序排序，这对于数据保持时间顺序至关重要\n",
    "        if re.match(check_name, filename) and os.path.isfile(files):\n",
    "\n",
    "            # using pandas to read csv file\n",
    "            # there are only three headers: Speed, Steering_angle, LiDAR_scan. \n",
    "            # But there are 1083 columns data, which means the number of header doesn't match number of column\n",
    "            # So, we need to skip first row, and set header is None.\n",
    "            # Besides, the value in last column is all None, we have to drop column at index -1\n",
    "            temp = pd.read_csv(files, skiprows=1, header=None).iloc[:, :-1]\n",
    "\n",
    "            # Because the speed is a very large number compare to steering angle,\n",
    "            # We have to normalize it into [-1, 1], and do the same to steering angle\n",
    "            # When apply the model in the simulator, remember to product corresponding value to the output of the model\n",
    "            # index 0 is speed, index 1 is steering angle\n",
    "            temp[0] = temp[0].map(lambda t : t/16.0)\n",
    "            temp[1] = temp[1].map(lambda t : t/0.192)\n",
    "\n",
    "            # append this csv file to the result, and turn it into numpy array with float format\n",
    "            datasets.append(np.array(temp, dtype=float))\n",
    "\n",
    "    # when append data to a list, there will be a copy of old list, which took a lot of memory\n",
    "    # but there is no reference to them, so we can call the garbage collector\n",
    "    gc.collect()\n",
    "\n",
    "    # datasets is a 3D list, shape 0 is number of csv files, shape 1 is number of rows in that csv file, shape 2 is 1083\n",
    "    return datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def loading_speed_steering_data(folder):\n",
    "\n",
    "    # file's name start with 'car_state_blue'\n",
    "    check_name = re.compile('^car_state_blue')\n",
    "\n",
    "    datasets = []\n",
    "\n",
    "    # Also need to iterate in sorted by name order\n",
    "    for filename in sorted(os.listdir(folder)):\n",
    "\n",
    "        files = os.path.join(folder, filename)\n",
    "\n",
    "        if re.match(check_name, filename) and os.path.isfile(files):\n",
    "\n",
    "            # We need another two columns from car_state_blue files,\n",
    "            # one is Velocity_X, another is Steering_angle\n",
    "            datasets.append(np.array(pd.read_csv(files).iloc[:, [3, 5]], dtype=float))\n",
    "\n",
    "    gc.collect()\n",
    "    # datasets is a 3D list, shape 0 is number of csv files, shape 1 is number of rows in that csv file, shape 2 is 2\n",
    "    return datasets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(tf.keras.layers.Layer):\n",
    "    def __init__(self, position, d_model):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.pos_encoding = self.positional_encoding(position, d_model)\n",
    "    # 位置编码主要包含两个参数：position最大位置数如1326(一次超车所用的时间数)  \n",
    "    # d_model时间特征的维度(每个时间点 所包含信息的长度 如1083)\n",
    "\n",
    "    #在 get_angles 方法中，为每个时间步的每个特征维度计算一个角度值。      \n",
    "    def get_angles(self, position, i, d_model):\n",
    "        # 计算一个超车实例中，时间点i位置的角度\n",
    "        angles = 1 / tf.pow(10000, (2 * (i // 2)) / tf.cast(d_model, tf.float32))\n",
    "        return position * angles # position是时间步索引，代表目前所处时间点，；angels基于时间步位置和特征维度索引计算出的角度值\n",
    "        # 序列中的每个时间步生成一个独特的角度值。这个角度值随着时间步的位置和特征维度的索引而变化，从而为每个时间步的每个特征维度生成唯一的编码。\n",
    "        # 这些角度值随后被用来生成正弦和余弦位置编码，这些编码最终被加到模型的输入特征上。\n",
    "    \n",
    "\n",
    "    \n",
    "    # 输入数据中的每个时间步（例如，在一个超车实例中的每个独立的测量点）都将获得一个唯一的位置编码。  //这些位置编码是通过在每个特征维度上应用正弦和余弦函数的不同频率来生成的。\n",
    "    # 对于给定的时间步，您的每个特征（在您的案例中为1083个特征，包括LiDAR数据、速度和转向角）都会根据其维度索引获得不同的角度值。\n",
    "    # 生成的位置编码随后被添加到原始输入特征中。这意味着，原始的特征集（每个时间步的1083个特征）现在被增强了额外的信息，表明了每个特征在序列中的位置。\n",
    "\n",
    "    def positional_encoding(self, position, d_model):\n",
    "        # 生成位置编码\n",
    "        angle_rads = self.get_angles(\n",
    "            position=tf.range(position, dtype=tf.float32)[:, tf.newaxis],\n",
    "            i=tf.range(d_model, dtype=tf.float32)[tf.newaxis, :],\n",
    "            d_model=d_model)\n",
    "        \n",
    "        # 将 sin 应用于数组中的偶数索引\n",
    "        sines = tf.math.sin(angle_rads[:, 0::2])\n",
    "\n",
    "        # 将 cos 应用于数组中的奇数索引\n",
    "        cosines = tf.math.cos(angle_rads[:, 1::2])\n",
    "\n",
    "        pos_encoding = tf.concat([sines, cosines], axis=-1)\n",
    "        pos_encoding = pos_encoding[tf.newaxis, ...]\n",
    "        return tf.cast(pos_encoding, tf.float32)\n",
    "    \n",
    "    \"\"\"\n",
    "    对于偶数索引的维度（例如，0, 2, 4, ...），应用正弦函数。\n",
    "    对于奇数索引的维度（例如，1, 3, 5, ...），应用余弦函数。\n",
    "    这样做的目的是在不同的维度间提供变化，使得位置编码更加丰富和多样化。\n",
    "    合并正弦和余弦编码:\n",
    "\n",
    "    正弦和余弦值被合并为一个完整的位置编码矩阵。这个矩阵的形状将是 [1, N, d_model]，其中 N 是时间步的数量，d_model 是特征维度的数量。\n",
    "    添加到输入特征:\n",
    "\n",
    "    这个位置编码矩阵随后在模型的 call 方法中被加到输入特征上。这意味着每个时间步的每个特征都会得到一个唯一的位置编码，从而使模型能够识别输入数据中时间步的顺序。       \n",
    "    \"\"\"\n",
    "\n",
    "    def call(self, inputs):\n",
    "        # 将位置编码添加到输入中\n",
    "        return inputs + self.pos_encoding[:, :tf.shape(inputs)[1], :]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_from_folders(folders):\n",
    "    all_datasets = []  # 这里假设所有数据集的数据将被合并到一个单一的列表中\n",
    "    all_speed_steering = []  # 同上，对于速度和转向数据\n",
    "    \n",
    "    for folder in folders:\n",
    "        # 加载当前文件夹的数据\n",
    "        datasets = loading_data(folder)\n",
    "        speed_steering = loading_speed_steering_data(folder)\n",
    "        \n",
    "        # 假设datasets和speed_steering是列表形式的数据，我们可以扩展现有列表以包含新加载的数据\n",
    "        all_datasets.extend(datasets)  # 扩展而不是附加以保持列表元素的平坦\n",
    "        all_speed_steering.extend(speed_steering)\n",
    "    \n",
    "    return all_datasets, all_speed_steering\n",
    "\n",
    "# 使用示例\n",
    "# folders = [\"/root/autodl-tmp/Dataset_shanghai\", \"/root/autodl-tmp/Australia_dataset\", \"/root/autodl-tmp/Gulf_dataset\", \"/root/autodl-tmp/Malaysian_dataset\"]\n",
    "folders = [\"/root/autodl-tmp/Dataset_shanghai\", \"/root/autodl-tmp/Australia_dataset\"]\n",
    "datasets, speed_steering = load_data_from_folders(folders)\n",
    "folders2 = [\"/root/autodl-tmp/Gulf_dataset\"]\n",
    "datasets_test, speed_steering_test = load_data_from_folders(folders2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerEncoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, num_heads, dff, d_model, rate):\n",
    "        super(TransformerEncoder, self).__init__()\n",
    "\n",
    "        self.mha = tf.keras.layers.MultiHeadAttention(num_heads=num_heads, key_dim=d_model)\n",
    "        self.ffn = tf.keras.Sequential([\n",
    "            tf.keras.layers.Dense(dff, activation='relu'),  # 第一层\n",
    "            tf.keras.layers.Dense(d_model)  # 第二层\n",
    "        ])\n",
    "\n",
    "        self.layernorm1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.layernorm2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout1 = tf.keras.layers.Dropout(rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(rate)\n",
    "\n",
    "    def call(self, x, mask=None, training=False):\n",
    "        attn_output = self.mha(x, x, x, attention_mask=mask)\n",
    "        # attn_output = self.mha(x, x, x)  # 自注意力机制\n",
    "        attn_output = self.dropout1(attn_output, training=training)\n",
    "        out1 = self.layernorm1(x + attn_output)\n",
    "\n",
    "        ffn_output = self.ffn(out1)  # 前馈网络\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        out2 = self.layernorm2(out1 + ffn_output)\n",
    "\n",
    "        return out2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you want to use part of the data, feel free to add bracket after datasets and speed_steering, like [0:100], make sure the size of sub data matches\n",
    "new_datasets = datasets\n",
    "new_speed_steering = speed_steering\n",
    "new_datasets_test = datasets_test\n",
    "new_speed_steering_test = speed_steering_test\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "special_value = -100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 【1083版本】\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "X_test = []\n",
    "Y_test = []\n",
    "\n",
    "\n",
    "for data, speed_steering in zip(new_datasets, new_speed_steering):\n",
    "    # 将LiDAR数据（除了前两列外的所有列）和速度、转向角数据（从new_speed_steering）合并\n",
    "    current_features = np.concatenate((data[:, 2:], speed_steering), axis=1)\n",
    "    X.append(current_features)\n",
    "\n",
    "    # 使用前两列作为要预测的目标\n",
    "    Y.append(data[:, 0:2])\n",
    "    \n",
    "for data, speed_steering in zip(new_datasets_test, new_speed_steering_test):\n",
    "    # 将LiDAR数据（除了前两列外的所有列）和速度、转向角数据（从new_speed_steering）合并\n",
    "    X_test.append(data[:, 2:])\n",
    "    # 使用前两列作为要预测的目标\n",
    "    Y_test.append(data[:, 0:2])\n",
    "    # current_features = np.concatenate((data[:, 2:], speed_steering), axis=1)\n",
    "    # X_combined.append(current_features)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# X = []\n",
    "# Y = []\n",
    "# X_combined = []\n",
    "\n",
    "# # First two columns are speed and steering angle where are the output, aka y value\n",
    "# # rest of columns are LiDAR data where are part of the input, aka X value\n",
    "# for data, speed_steering in zip(new_datasets, new_speed_steering):\n",
    "#     # 将LiDAR数据（除了前两列外的所有列）和速度、转向角数据（从new_speed_steering）合并\n",
    "#     X.append(data[:, 2:])\n",
    "#     # 使用前两列作为要预测的目标\n",
    "#     Y.append(data[:, 0:2])\n",
    "#     # current_features = np.concatenate((data[:, 2:], speed_steering), axis=1)\n",
    "#     # X_combined.append(current_features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 下一步就是确定这个X可不可以放到Transformer中，可能还要加上时间戳或者什么 确保能分别开\n",
    "- 再一个就是我还不清楚现在的XY分别张什么样子，需要打印出来看一下\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- X列表，有193个数组，这代表着上海赛道数据集的193次超车实例 / 每一个数组都是二维数组，一个数组有一行 \n",
    "- 一行中有（1326 ... 739 取决于这个超车实例所用的时间）个子数组 \n",
    "- 每个子数组代表着超车过程中的一个时间节点，里面包含了该时间节点我们控制的超车车辆-小蓝车的手柄端速度信息，转向角信息，雷达信息；"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 【1083版本】\n",
    "# # 示例：选择和处理较短的序列 进行模型测试\n",
    "\n",
    "# # # 截断长度大于4000的序列\n",
    "# X_trimmed = [x if len(x) <= 4000 else x[:4000] for x in X]\n",
    "# Y_trimmed = [y if len(y) <= 4000 else y[:4000] for y in Y]\n",
    "\n",
    "# # # 填充至最大长度\n",
    "# Xpad = tensorflow.keras.preprocessing.sequence.pad_sequences(X_trimmed, padding='post', value=special_value)\n",
    "# Ypad = tensorflow.keras.preprocessing.sequence.pad_sequences(Y_trimmed, padding='post', value=special_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 示例：选择和处理较短的序列 进行模型测试\n",
    "\n",
    "# # # 截断长度大于4000的序列\n",
    "# X_trimmed = [x if len(x) <= 2000 else x[:2000] for x in X]\n",
    "# Y_trimmed = [y if len(y) <= 2000 else y[:2000] for y in Y]\n",
    "# # X_combined_trimmed = [z if len(z) <= 4000 else z[:4000] for z in X_combined]\n",
    "\n",
    "# # # 填充至最大长度\n",
    "# Xpad = tensorflow.keras.preprocessing.sequence.pad_sequences(X_trimmed, padding='post', value=special_value)\n",
    "# Ypad = tensorflow.keras.preprocessing.sequence.pad_sequences(Y_trimmed, padding='post', value=special_value)\n",
    "\n",
    "# # X_combined_pad = tensorflow.keras.preprocessing.sequence.pad_sequences(X_combined_trimmed, padding='post', value=special_value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_sequences(sequences, window_size, step):\n",
    "    windows = []\n",
    "    for seq in sequences:\n",
    "        for start in range(0, len(seq) - window_size + 1, step):\n",
    "            end = start + window_size\n",
    "            window = seq[start:end]\n",
    "            windows.append(window)\n",
    "    return windows\n",
    "\n",
    "# 设置窗口大小和步长\n",
    "window_size = 600  # 例如，每个窗口500个时间步\n",
    "step = 500  # 步长设置为400，提供100个时间步的重叠\n",
    "\n",
    "# 对 X 和 Y 应用分割函数\n",
    "X_split = split_sequences(X, window_size, step)\n",
    "Y_split = split_sequences(Y, window_size, step)\n",
    "\n",
    "# 转换为适合模型输入的格式\n",
    "Xpad = tensorflow.keras.preprocessing.sequence.pad_sequences(X_split, padding=\"post\", value=special_value)\n",
    "Ypad = tensorflow.keras.preprocessing.sequence.pad_sequences(Y_split, padding=\"post\", value=special_value)\n",
    "\n",
    "\n",
    "# 对 X 和 Y 应用分割函数\n",
    "X_test_split = split_sequences(X_test, window_size, step)\n",
    "Y_test_split = split_sequences(Y_test, window_size, step)\n",
    "\n",
    "# 转换为适合模型输入的格式\n",
    "Xpad_test = tensorflow.keras.preprocessing.sequence.pad_sequences(X_test_split, padding=\"post\", value=special_value)\n",
    "Ypad_test = tensorflow.keras.preprocessing.sequence.pad_sequences(Y_test_split, padding=\"post\", value=special_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_size = 1083\n",
    "num_heads = 4\n",
    "# 一个head就有100m左右，是影响模型大小的最重要的参数\n",
    "dff = 2048\n",
    "num_layers = 2\n",
    "dropout_rate = 0.15\n",
    "# 这个也影响文件大小"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, None, 1083)]      0         \n",
      "                                                                 \n",
      " masking (Masking)           (None, None, 1083)        0         \n",
      "                                                                 \n",
      " positional_encoding (Positi  (None, None, 1083)       0         \n",
      " onalEncoding)                                                   \n",
      "                                                                 \n",
      " transformer_encoder (Transf  (None, None, 1083)       23223734  \n",
      " ormerEncoder)                                                   \n",
      "                                                                 \n",
      " transformer_encoder_1 (Tran  (None, None, 1083)       23223734  \n",
      " sformerEncoder)                                                 \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, None, 2)           2168      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 46,449,636\n",
      "Trainable params: 46,449,636\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 【1083版本】\n",
    "# 定义模型输入\n",
    "with strategy.scope():\n",
    "    # inputs = tf.keras.Input(shape=(None, feature_size))\n",
    "    inputs = Input(shape=(None, feature_size))\n",
    "\n",
    "    # 添加 Masking 层\n",
    "    masking_layer = Masking(mask_value=special_value)\n",
    "    x = masking_layer(inputs)\n",
    "\n",
    "    # pos_encoding_layer = PositionalEncoding(position=4000, d_model=feature_size_after_conv)\n",
    "    pos_encoding_layer = PositionalEncoding(position=600, d_model=feature_size)\n",
    "    x = pos_encoding_layer(x)\n",
    "\n",
    "    # 添加 Transformer 编码器层\n",
    "    for _ in range(num_layers):\n",
    "        # x = TransformerEncoder(num_heads=num_heads, dff=dff, d_model=feature_size_after_conv, rate=dropout_rate)(x)\n",
    "        x = TransformerEncoder(num_heads=num_heads, dff=dff, d_model=feature_size, rate=dropout_rate)(x)\n",
    "\n",
    "    # 添加输出层\n",
    "    outputs = Dense(2, activation='linear')(x)\n",
    "    \n",
    "    \n",
    "    # 创建模型\n",
    "    model = Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "    \n",
    "    # 定义模型\n",
    "    # model = tf.keras.Model(inputs=inputs, outputs=outputs)\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # 定义模型输入\n",
    "# inputA = tf.keras.Input(shape=(None, 1081))\n",
    "\n",
    "# # 添加 Masking 层\n",
    "# masking_layer = Masking(mask_value=special_value)\n",
    "# x = masking_layer(inputA)\n",
    "\n",
    "\n",
    "# x = tf.keras.layers.Conv1D(filters=32, kernel_size=3, activation='relu', padding='same')(x)\n",
    "# # 对于1081雷达数据做预处理\n",
    "\n",
    "\n",
    "\n",
    "# inputB = tf.keras.Input(shape=(None, 2))\n",
    "\n",
    "# # Still need another Masking layer\n",
    "# B = masking_layer(inputB)\n",
    "\n",
    "# x = concatenate([x, B], axis=2)\n",
    "# # 合并处理完的数据 放入模型输入层\n",
    "\n",
    "# feature_size_after_conv = 34\n",
    "\n",
    "# pos_encoding_layer = PositionalEncoding(position=4000, d_model=feature_size_after_conv)\n",
    "# x = pos_encoding_layer(x)\n",
    "\n",
    "# # 添加 Transformer 编码器层\n",
    "# for _ in range(num_layers):\n",
    "#     x = TransformerEncoder(num_heads=num_heads, dff=dff, d_model=feature_size_after_conv, rate=dropout_rate)(x)\n",
    "#     # x = TransformerEncoder(num_heads=num_heads, dff=dff, d_model=feature_size, rate=dropout_rate)(x)\n",
    "\n",
    "# # 添加输出层\n",
    "# outputs = tf.keras.layers.Dense(2, activation='linear')(x)\n",
    "\n",
    "# # 定义模型\n",
    "# Transformer_model = tf.keras.Model(inputs=[inputA, inputB], outputs=outputs)\n",
    "\n",
    "# Transformer_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1332"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "del datasets\n",
    "del speed_steering\n",
    "gc.collect()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    }
   ],
   "source": [
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss='mean_squared_error',\n",
    "    metrics=['mae']\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 03:47:45.900786: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_2\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"TensorDataset/_1\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_flat_map_fn_1142\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\020FlatMapDataset:1\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT32\n",
      "      type: DT_INT32\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT32\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/13\n",
      "INFO:tensorflow:batch_all_reduce: 34 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n",
      "INFO:tensorflow:batch_all_reduce: 34 all-reduces with algorithm = nccl, num_packs = 1\n",
      "INFO:tensorflow:Reduce to /job:localhost/replica:0/task:0/device:CPU:0 then broadcast to ('/job:localhost/replica:0/task:0/device:CPU:0',).\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 03:47:51.604247: I tensorflow/stream_executor/cuda/cuda_blas.cc:1786] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1241/1242 [============================>.] - ETA: 0s - loss: 1.0880 - mae: 0.3715"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-03-30 03:48:53.064845: W tensorflow/core/grappler/optimizers/data/auto_shard.cc:776] AUTO sharding policy will apply DATA sharding policy as it failed to apply FILE sharding policy because of the following reason: Did not find a shardable source, walked to a node which is not a dataset: name: \"FlatMapDataset/_9\"\n",
      "op: \"FlatMapDataset\"\n",
      "input: \"PrefetchDataset/_8\"\n",
      "attr {\n",
      "  key: \"Targuments\"\n",
      "  value {\n",
      "    list {\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"_cardinality\"\n",
      "  value {\n",
      "    i: -2\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"f\"\n",
      "  value {\n",
      "    func {\n",
      "      name: \"__inference_Dataset_flat_map_slice_batch_indices_13980\"\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"metadata\"\n",
      "  value {\n",
      "    s: \"\\n\\021FlatMapDataset:32\"\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_shapes\"\n",
      "  value {\n",
      "    list {\n",
      "      shape {\n",
      "        dim {\n",
      "          size: -1\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "attr {\n",
      "  key: \"output_types\"\n",
      "  value {\n",
      "    list {\n",
      "      type: DT_INT64\n",
      "    }\n",
      "  }\n",
      "}\n",
      "experimental_type {\n",
      "  type_id: TFT_PRODUCT\n",
      "  args {\n",
      "    type_id: TFT_DATASET\n",
      "    args {\n",
      "      type_id: TFT_PRODUCT\n",
      "      args {\n",
      "        type_id: TFT_TENSOR\n",
      "        args {\n",
      "          type_id: TFT_INT64\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      ". Consider either turning off auto-sharding or switching the auto_shard_policy to DATA to shard this dataset. You can do this by creating a new `tf.data.Options()` object then setting `options.experimental_distribute.auto_shard_policy = AutoShardPolicy.DATA` before applying the options object to the dataset via `dataset.with_options(options)`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1242/1242 [==============================] - 72s 54ms/step - loss: 1.0872 - mae: 0.3714 - val_loss: 0.1993 - val_mae: 0.3242\n",
      "Epoch 2/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.1066 - mae: 0.2043 - val_loss: 0.1964 - val_mae: 0.3572\n",
      "Epoch 3/13\n",
      "1242/1242 [==============================] - 62s 50ms/step - loss: 0.0956 - mae: 0.1825 - val_loss: 0.1007 - val_mae: 0.1687\n",
      "Epoch 4/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0873 - mae: 0.1613 - val_loss: 0.0933 - val_mae: 0.2308\n",
      "Epoch 5/13\n",
      "1242/1242 [==============================] - 62s 50ms/step - loss: 0.0835 - mae: 0.1481 - val_loss: 0.0757 - val_mae: 0.1072\n",
      "Epoch 6/13\n",
      "1242/1242 [==============================] - 62s 50ms/step - loss: 0.0792 - mae: 0.1350 - val_loss: 0.0740 - val_mae: 0.1272\n",
      "Epoch 7/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0764 - mae: 0.1279 - val_loss: 0.0763 - val_mae: 0.1217\n",
      "Epoch 8/13\n",
      "1242/1242 [==============================] - 62s 50ms/step - loss: 0.0757 - mae: 0.1208 - val_loss: 0.0753 - val_mae: 0.0986\n",
      "Epoch 9/13\n",
      "1242/1242 [==============================] - 62s 50ms/step - loss: 0.0755 - mae: 0.1208 - val_loss: 0.0802 - val_mae: 0.1516\n",
      "Epoch 10/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0758 - mae: 0.1215 - val_loss: 0.0761 - val_mae: 0.1282\n",
      "Epoch 11/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0752 - mae: 0.1192 - val_loss: 0.0782 - val_mae: 0.1388\n",
      "Epoch 12/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0759 - mae: 0.1207 - val_loss: 0.0850 - val_mae: 0.1653\n",
      "Epoch 13/13\n",
      "1242/1242 [==============================] - 61s 49ms/step - loss: 0.0752 - mae: 0.1198 - val_loss: 0.0832 - val_mae: 0.1277\n"
     ]
    }
   ],
   "source": [
    "train_generator = MemoryDataGenerator(Xpad, Ypad, batch_size=1)\n",
    "\n",
    "# 训练模型\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    epochs=13,  # 举例\n",
    "    # batch_size=8,  # 举例\n",
    "    validation_data=(Xpad, Ypad)\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Found untraced functions such as multi_head_attention_layer_call_fn, multi_head_attention_layer_call_and_return_conditional_losses, layer_normalization_layer_call_fn, layer_normalization_layer_call_and_return_conditional_losses, layer_normalization_1_layer_call_fn while saving (showing 5 of 44). These functions will not be directly callable after loading.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /root/autodl-tmp/models/Transformer_model/12_short_sequence_9_adjust_500/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /root/autodl-tmp/models/Transformer_model/12_short_sequence_9_adjust_500/assets\n"
     ]
    }
   ],
   "source": [
    "model.save(\"/root/autodl-tmp/models/Transformer_model/12_short_sequence_9_adjust_500/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model = load_model(\"/root/autodl-tmp/models/Transformer_model/12_short_sequence/\")\n",
    "# model.summary()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
